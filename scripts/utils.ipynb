{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3ef1070-5b4c-4c3a-891f-46b6757821e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import openai\n",
    "import json\n",
    "import re\n",
    "import tqdm\n",
    "import pickle\n",
    "import hnswlib\n",
    "import copy\n",
    "import torch\n",
    "import accelerate\n",
    "import os\n",
    "import torch.nn as nn\n",
    "\n",
    "import pytrec_eval\n",
    "\n",
    "if torch.backends.cuda.is_built():\n",
    "    DEVICE = 'cuda:0'\n",
    "else:\n",
    "    DEVICE = 'cpu'\n",
    "\n",
    "import random\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "\n",
    "\n",
    "from transformers import pipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from sentence_transformers import SentenceTransformer, util, CrossEncoder\n",
    "from collections import Counter\n",
    "\n",
    "# For parallize calling chatGPT\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import concurrent\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', FutureWarning)\n",
    "\n",
    "AUTHOR2EMBEDDING = json.load(open(\"../data/AUTHOR2EMBEDDING.json\"))\n",
    "EID2EMBEDDING = json.load(open(\"../data/EID2EMBEDDING.json\"))\n",
    "\n",
    "\n",
    "class QueryExpansion():\n",
    "    def __init__(self, query2doc: dict, folderName: str, top_k: int = 1000, maxWorker: int = 5, maxTokens: int = 800,\n",
    "                 maxAbsLens: int = 8000, ifRankAbs: bool = False):\n",
    "        self.data = query2doc\n",
    "\n",
    "        ## folder for store result\n",
    "        self.path = '../results'\n",
    "        \n",
    "        if not os.path.exists(self.path):\n",
    "            os.makedirs(self.path)\n",
    "            print(\"Path created:\", self.path)\n",
    "        else:\n",
    "            print(\"Path already exists:\", self.path)\n",
    "\n",
    "        ## set up open AI\n",
    "        openai.api_base = \"https://api.openai.com/v1\"\n",
    "        openai.api_key = 'your open ai api key'\n",
    "\n",
    "        self.maxTokens = maxTokens\n",
    "        self.maxAbsLens = maxAbsLens\n",
    "        self.ifRankAbs = ifRankAbs\n",
    "\n",
    "        ## set search index\n",
    "        self.top_k = top_k\n",
    "        self.searching_index = hnswlib.Index(space='cosine', dim=384)\n",
    "        self.searching_index.load_index('../data/hnsw_index.idx')\n",
    "        self.searching_index.set_ef(int(self.top_k * 1.5))\n",
    "\n",
    "        ## embedding model\n",
    "        self.model = SentenceTransformer('all-MiniLM-L12-v2', device=DEVICE)\n",
    "\n",
    "        ## LLM expansion\n",
    "        self.max_workers = maxWorker\n",
    "        self.overall_result = pd.DataFrame({'Items': [], 'MAP@10': [], 'NDCG@10': [], 'NDCG@20': [], \\\n",
    "                                            'Recall@10': [], 'Recall@50': [], 'Recall@200': [], 'Precision@10': [],'Precision@100': [], 'MRR': []})\n",
    "        self.result = pd.DataFrame({'Query': [], 'Query NDCG@10': [], \\\n",
    "                                    'Expanded Query': [], 'Expanded Query NDCG@10': [], \\\n",
    "                                    'Personalised Expanded Query 1': [], 'Personalised Expanded Query 1 NDCG@10': [], \\\n",
    "                                    'Personalised Expanded Query 2': [], 'Personalised Expanded Query 2 NDCG@10': [], \\\n",
    "                                    'Personalised Expanded Query 3': [], 'Personalised Expanded Query 3 NDCG@10': [], \\\n",
    "                                    'Personalised Expanded Query (include all keywords)': [],\n",
    "                                    'Personalised Expanded Query (include all keywords) NDCG@10': [], 'Personalised Expanded Query NDCG@10': []})\n",
    "        \n",
    "\n",
    "    def clear_result(self):\n",
    "        self.overall_result = pd.DataFrame({'Items': [], 'MAP@10': [], 'NDCG@10': [], 'NDCG@20': [], \\\n",
    "                                            'Recall@10': [], 'Recall@50': [], 'Recall@200': [], 'Relavance@10': []})\n",
    "\n",
    "    def F_call_chatGPT(self, system_message: str, prompt: str):\n",
    "        message_text = [{\"role\": \"system\", \"content\": system_message}, {\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "        for attempt in range(10):\n",
    "            try:\n",
    "                completion = openai.ChatCompletion.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=message_text,\n",
    "                temperature=1,\n",
    "                max_tokens=self.maxTokens,\n",
    "                top_p=1e-16,\n",
    "                n=1,\n",
    "                logprobs=False,\n",
    "                frequency_penalty=0,\n",
    "                presence_penalty=0,\n",
    "                stop=None\n",
    "                )\n",
    "\n",
    "                try:\n",
    "                    return (completion['choices'][0]['message']['content'])\n",
    "                except:\n",
    "                    return \"LLMs Failed\"\n",
    "            except openai.OpenAIError as error:\n",
    "                if attempt < 5 - 1:\n",
    "                    time.sleep(60)\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    def evalPerformance(self, query_2_click_doc: dict, query_key: str):\n",
    "        queries = []\n",
    "        keys = []\n",
    "        for sid_query in query_2_click_doc:\n",
    "            queries.append(query_2_click_doc[sid_query][query_key])\n",
    "            keys.append(sid_query)\n",
    "\n",
    "        embs = self.model.encode(queries, batch_size=128, show_progress_bar=False, device=DEVICE)\n",
    "        query_2_embedding = dict(zip(keys, embs))\n",
    "\n",
    "        del queries\n",
    "        del embs\n",
    "\n",
    "        # output variables\n",
    "        run = defaultdict(dict)\n",
    "        qrel = defaultdict(dict)\n",
    "\n",
    "        for sidquery in list(query_2_click_doc.keys()):\n",
    "\n",
    "            # query embedding\n",
    "            query_vector = query_2_embedding[sidquery]\n",
    "            query_vector = query_vector.astype('float32')\n",
    "\n",
    "            labels, distances = self.searching_index.knn_query(query_vector, k=self.top_k)\n",
    "            \n",
    "            # output prediction\n",
    "            for i in range(self.top_k):\n",
    "                run[sidquery][str(labels[0][i])] = self.top_k - i\n",
    "\n",
    "            # for i in range(len(labels[0])):\n",
    "            # run[sidquery][str(labels[0][i])] = 1 - distances[0][i]\n",
    "\n",
    "\n",
    "            # output ground truth\n",
    "            for eid, rank in query_2_click_doc[sidquery]['click_doc_list']:\n",
    "                qrel[sidquery][str(eid)] = 1\n",
    "\n",
    "        evaluator = pytrec_eval.RelevanceEvaluator(qrel, {'map_cut_10', 'ndcg_cut_10', 'ndcg_cut_20', \\\n",
    "                                                    'recall_10', 'recall_50', 'recall_200', 'P_10', 'P_100', 'recip_rank'})\n",
    "        result = evaluator.evaluate(run)\n",
    "        avg_map = round(sum([sub_dict['map_cut_10'] for sub_dict in result.values()]) / len(result), 3)\n",
    "        avg_ndcg_10 = round(sum([sub_dict['ndcg_cut_10'] for sub_dict in result.values()]) / len(result), 3)\n",
    "        avg_ndcg_20 = round(sum([sub_dict['ndcg_cut_20'] for sub_dict in result.values()]) / len(result), 3)\n",
    "        avg_recall_10 = round(sum([sub_dict['recall_10'] for sub_dict in result.values()]) / len(result), 3)\n",
    "        avg_recall_50 = round(sum([sub_dict['recall_50'] for sub_dict in result.values()]) / len(result), 3)\n",
    "        avg_recall_200 = round(sum([sub_dict['recall_200'] for sub_dict in result.values()]) / len(result), 3)\n",
    "        avg_precision_10 = round(sum([sub_dict['P_10'] for sub_dict in result.values()]) / len(result), 3)\n",
    "        avg_precision_100 = round(sum([sub_dict['P_100'] for sub_dict in result.values()]) / len(result), 3)\n",
    "        avg_mrr = round(sum([sub_dict['recip_rank'] for sub_dict in result.values()]) / len(result), 3)\n",
    "\n",
    "        # print(f\"MAP@10: {avg_map}, NDCG@10: {avg_ndcg_10}, NDCG@20: {avg_ndcg_20}, Recall@10:{avg_recall_10}, @50: {avg_recall_50}, @100: {avg_recall_100}\")\n",
    "        overall_result = {'MAP@10': avg_map, 'NDCG@10': avg_ndcg_10, 'NDCG@20': avg_ndcg_20, \\\n",
    "                          'Recall@10': avg_recall_10, 'Recall@50': avg_recall_50, 'Recall@200': avg_recall_200, 'Precision@10': avg_precision_10, 'Precision@100': avg_precision_100, 'MRR': avg_mrr}\n",
    "\n",
    "        return result, overall_result\n",
    "    \n",
    "\n",
    "    def cosine_similarity(self, vector1, vector2):\n",
    "        dot_product = np.dot(vector1, vector2)\n",
    "        norm_vector1 = np.linalg.norm(vector1)\n",
    "        norm_vector2 = np.linalg.norm(vector2)\n",
    "        similarity = dot_product / (norm_vector1 * norm_vector2)\n",
    "        return similarity\n",
    "\n",
    "    def rerank(self, emb, expanded_emb, depth):\n",
    "        labels, distances = self.searching_index.knn_query(emb, k=self.top_k)\n",
    "        new_distances = []\n",
    "        for label in labels[0][:depth]:\n",
    "            doc_emb = self.searching_index.get_items([label])[0]\n",
    "            new_distances.append(1 - self.cosine_similarity(expanded_emb, doc_emb))\n",
    "        sorted_indices = np.argsort(np.array(new_distances))\n",
    "        reranked_distances = np.concatenate((np.array(new_distances)[sorted_indices], distances[0][depth:]))\n",
    "        reranked_label = np.concatenate((labels[0][:depth][sorted_indices], labels[0][depth:]))\n",
    "        return reranked_label.reshape((1, -1)), reranked_distances.reshape(1, -1)\n",
    "\n",
    "    def evalRerankPerformance(self, query_2_click_doc: dict, query_key: str, depth: int):\n",
    "        queries = []\n",
    "        expanded_queries = []\n",
    "        keys = []\n",
    "        for sid_query in query_2_click_doc:\n",
    "            queries.append(query_2_click_doc[sid_query]['query'])\n",
    "            expanded_queries.append(query_2_click_doc[sid_query][query_key])\n",
    "            keys.append(sid_query)\n",
    "        embs = self.model.encode(queries, batch_size=128, show_progress_bar=False, device=DEVICE)\n",
    "        embs_expanded = self.model.encode(expanded_queries, batch_size=128, show_progress_bar=False, device=DEVICE)\n",
    "        # print(np.sum(embs - embs_expanded))\n",
    "\n",
    "        query_2_embedding = dict(zip(keys, embs))\n",
    "        expanded_query_2_embedding = dict(zip(keys, embs_expanded))\n",
    "\n",
    "        del queries\n",
    "        del expanded_queries\n",
    "        del embs_expanded\n",
    "        del embs\n",
    "\n",
    "        # output variables\n",
    "        run = defaultdict(dict)\n",
    "        qrel = defaultdict(dict)\n",
    "\n",
    "        relevances = []\n",
    "\n",
    "        for sidquery in list(query_2_click_doc.keys()):\n",
    "\n",
    "            # query embedding\n",
    "            query_vector = query_2_embedding[sidquery]\n",
    "            query_vector = query_vector.astype('float32')\n",
    "            expanded_query_vector = expanded_query_2_embedding[sidquery]\n",
    "            expanded_query_vector = expanded_query_vector.astype('float32')\n",
    "\n",
    "            labels, distances = self.rerank(query_vector, expanded_query_vector, depth)\n",
    "\n",
    "            # output prediction\n",
    "            for i in range(self.top_k):\n",
    "                run[sidquery][str(labels[0][i])] = self.top_k - i\n",
    "            \n",
    "\n",
    "            # output ground truth\n",
    "            for eid, rank in query_2_click_doc[sidquery]['click_doc_list']:\n",
    "                qrel[sidquery][str(eid)] = 1\n",
    "\n",
    "        evaluator = pytrec_eval.RelevanceEvaluator(qrel, {'map_cut_10', 'ndcg_cut_10', 'ndcg_cut_20', \\\n",
    "                                                          'recall_10', 'recall_50', 'recall_200', 'P_10', 'P_100', 'recip_rank'})\n",
    "        result = evaluator.evaluate(run)\n",
    "\n",
    "        avg_map = round(sum([sub_dict['map_cut_10'] for sub_dict in result.values()]) / len(result), 3)\n",
    "        avg_ndcg_10 = round(sum([sub_dict['ndcg_cut_10'] for sub_dict in result.values()]) / len(result), 3)\n",
    "        avg_ndcg_20 = round(sum([sub_dict['ndcg_cut_20'] for sub_dict in result.values()]) / len(result), 3)\n",
    "        avg_recall_10 = round(sum([sub_dict['recall_10'] for sub_dict in result.values()]) / len(result), 3)\n",
    "        avg_recall_50 = round(sum([sub_dict['recall_50'] for sub_dict in result.values()]) / len(result), 3)\n",
    "        avg_recall_200 = round(sum([sub_dict['recall_200'] for sub_dict in result.values()]) / len(result), 3)\n",
    "        avg_precision_10 = round(sum([sub_dict['P_10'] for sub_dict in result.values()]) / len(result), 3)\n",
    "        avg_precision_100 = round(sum([sub_dict['P_100'] for sub_dict in result.values()]) / len(result), 3)\n",
    "        avg_mrr = round(sum([sub_dict['recip_rank'] for sub_dict in result.values()]) / len(result), 3)\n",
    "\n",
    "\n",
    "        # print(f\"MAP@10: {avg_map}, NDCG@10: {avg_ndcg_10}, NDCG@20: {avg_ndcg_20}, Recall@10:{avg_recall_10}, @50: {avg_recall_50}, @100: {avg_recall_100}\")\n",
    "        overall_result = {'MAP@10': avg_map, 'NDCG@10': avg_ndcg_10, 'NDCG@20': avg_ndcg_20, \\\n",
    "                          'Recall@10': avg_recall_10, 'Recall@50': avg_recall_50, 'Recall@200': avg_recall_200, 'Precision@10': avg_precision_10, 'Precision@100': avg_precision_100, 'MRR': avg_mrr}\n",
    "\n",
    "        return result, overall_result\n",
    "\n",
    "    def LLMAuthorRepresentation(self, docs_abs: list):\n",
    "        author_abstracts = ''\n",
    "        for i_abs, abs in enumerate(docs_abs):\n",
    "            author_abstracts += f\"\\tAbstract {i_abs + 1}: {abs}\\n\"\n",
    "            if len(author_abstracts) > self.maxAbsLens:\n",
    "                break\n",
    "\n",
    "        system_message = \"\"\"You are an AI assistant specialized in providing a list of keywords that represent an author's field based on a list of abstracts from the author's publications. \n",
    "    The output should be in JSON format, containing one key: 'keywords'. \n",
    "    The value of 'keywords' should be a list of keywords with a length of less than 10. \n",
    "    Also, please note that no explanation about how these keywords are obtained is required.\n",
    "    PLEASE DO NOT PROVIDE ANY EXPLANATION IN THE OUTPUT! PLEASE DO NOT INCLUDE ANY OTHER INFORMATION!!!\n",
    "    Please DO NOT GIVE ANY OTHER TEXT, ONLY GIVE ME ONE JSON OUTPUT!\n",
    "    PLEASE ONLY GIVE ME ONE JSON OUTPUT!\n",
    "    \"\"\"\n",
    "        count = 0\n",
    "        while count <= 2:\n",
    "            json_string = self.F_call_chatGPT(system_message, author_abstracts)\n",
    "            try:\n",
    "                author_keywords = json.loads(json_string)['keywords']\n",
    "                return author_keywords\n",
    "            except:\n",
    "                count += 1\n",
    "                continue\n",
    "        return json_string\n",
    "\n",
    "    def LLMExpansion(self, query: str, author_keywords: list):\n",
    "        system_message = \"\"\"\n",
    "    You are an AI assistant specialized in refining search queries for scientific documents. Your task is to expand a given query with the top three phrases that are most relevant to the original query and replace all acronyms with their full forms from the user’s domain of expertise. Each expansion must not contain the original acronym but its full form. The user’s expertise is represented by specific keywords. However, expansions must be directly related to the original query, even if they match the user’s expertise. Provide a brief rationale for your choices. Your output should be a JSON object with two keys: ‘rationale’ and ‘expansion’. The ‘expansion’ should include a list of the top THREE related phrases and full forms of acronyms. Remember, only include a short rationale and avoid any additional explanation in your output.\n",
    "    \"\"\"\n",
    "        prompt = f\"\"\"\n",
    "    Given the query and user representation below, expand the query with the top three phrases that are most relevant to the original query and replace any acronyms with their full forms from the user’s domain of expertise. Even if phrases match the user’s expertise, ensure they are directly related to the original query and do not contain the original acronym. If the query aligns with the user’s expertise, prioritize it; if not, focus on general expansion.\n",
    "    Query: {query}\n",
    "    User Representation: {author_keywords}\n",
    "    \"\"\"\n",
    "\n",
    "        count = 0\n",
    "        while count <= 2:\n",
    "            expanded_query_GPT_answer = self.F_call_chatGPT(system_message, prompt)\n",
    "            try:\n",
    "                expanded_query = json.loads(expanded_query_GPT_answer)['expansion']\n",
    "                return expanded_query\n",
    "            except:\n",
    "                count += 1\n",
    "                continue\n",
    "        return expanded_query_GPT_answer\n",
    "    \n",
    "    def LLMAnswer(self, query: str):\n",
    "        system_message = \"\"\"You are an AI assistant to answer query.\n",
    "    Your output should be a JSON array with two keys: \"rationale\" and \"answer\". The \"answer\" should answer the input query. Remember, only include a short rationale and avoid any additional explanation in your output.\n",
    "    \"\"\"\n",
    "\n",
    "        prompt = f\"\"\"Answer the following question:\n",
    "    query: {query} \n",
    "    Give the rationale before answering.\n",
    "    \"\"\"\n",
    "        # count = 0\n",
    "        # while count <= 2:\n",
    "        expanded_query_GPT_answer = self.F_call_chatGPT(system_message, prompt)\n",
    "        try:\n",
    "            expanded_query = json.loads(expanded_query_GPT_answer)['answer']\n",
    "            return expanded_query\n",
    "        except:\n",
    "            return expanded_query_GPT_answer\n",
    "\n",
    " \n",
    "\n",
    "    def getABS(self, query: str, authorID: int):\n",
    "        queryEmb = self.model.encode(query, device=DEVICE)\n",
    "        docs = AUTHOR2EMBEDDING[str(authorID)]['eids']\n",
    "\n",
    "        contents = [EID2EMBEDDING[eid]['content'] for eid in docs]\n",
    "        docEmbs = [EID2EMBEDDING[eid]['embedding'] for eid in docs]\n",
    "\n",
    "        docDistances = [1 - self.cosine_similarity(queryEmb, docEmb) for docEmb in docEmbs]\n",
    "\n",
    "        sorted_lists = sorted(zip(docDistances, contents))\n",
    "        docDistances, contents = zip(*sorted_lists)\n",
    "\n",
    "        return [contents[0]]\n",
    "\n",
    "    def extractAuthorRepresentation(self):\n",
    "        df_scopus_user_dict = {}\n",
    "        for key, value in self.data.items():\n",
    "            if self.ifRankAbs:\n",
    "                df_scopus_user_dict[key] = self.getABS(value['query'], value['author_id'])\n",
    "            else:\n",
    "                docs = AUTHOR2EMBEDDING[value['author_id']]['eids']\n",
    "                df_scopus_user_dict[key] = [EID2EMBEDDING[eid]['content'] for eid in docs]\n",
    "        \n",
    "        # for key, value in self.data.items():\n",
    "        #     try:\n",
    "        #         value['author_keywords']\n",
    "        #     except:\n",
    "        #         value['author_keywords'] = self.LLMAuthorRepresentation(df_scopus_user_dict[key])\n",
    "        \n",
    "        def process_item(key, value):\n",
    "            try:\n",
    "                value['author_keywords']\n",
    "            except:\n",
    "                value['author_keywords'] = self.LLMAuthorRepresentation(df_scopus_user_dict[key])\n",
    "            return 0\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
    "            futures = [executor.submit(process_item, key, value) for key, value in self.data.items()]\n",
    "            for future in tqdm.tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc='Processing'):\n",
    "                future.result()\n",
    "\n",
    "    def personalisedExpandQuery(self):\n",
    "        def process_item(key, value):\n",
    "            try:\n",
    "                value['expanded_keywords']\n",
    "            except:\n",
    "                expanded_keywords = self.LLMExpansion(value['query'], value['author_keywords'])\n",
    "                value['expanded_keywords'] = expanded_keywords\n",
    "                time.sleep(0.1)\n",
    "            return 0\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
    "            futures = [executor.submit(process_item, key, value) for key, value in self.data.items()]\n",
    "            for future in tqdm.tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc='Processing'):\n",
    "                future.result()\n",
    "\n",
    "    def expandQuery(self):\n",
    "        def process_item(key, value):\n",
    "            try:\n",
    "                value['answer_query']\n",
    "            except:\n",
    "                expanded_query = self.LLMAnswer(value['query'])\n",
    "                value['answer_query'] = expanded_query\n",
    "                time.sleep(0.1)\n",
    "            return 0\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
    "            futures = [executor.submit(process_item, key, value) for key, value in self.data.items()]\n",
    "            for future in tqdm.tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc='Processing'):\n",
    "                future.result()\n",
    "\n",
    "    def storeData(self, filename: str):\n",
    "        with open(self.path+filename, 'wb') as file:\n",
    "            pickle.dump(self.data, file)\n",
    "\n",
    "    def readData(self, filename: str):\n",
    "        with open(self.path+filename, 'rb') as file:\n",
    "            self.data = pickle.load(file)\n",
    "\n",
    "    def writeReadMe(self, text: str):\n",
    "        with open(self.path+'README.txt', 'w') as file:\n",
    "            file.write(text)\n",
    "\n",
    "    \n",
    "    def evalRerunkExpandedPerformance(self, query_2_click_doc: dict, depth: int):\n",
    "        queries = []\n",
    "        expandedQuery1 = []\n",
    "        expandedQuery2 = []\n",
    "        expandedQuery3 = []\n",
    "        keys = []\n",
    "        for sid_query in query_2_click_doc:\n",
    "            query = query_2_click_doc[sid_query]['query']\n",
    "            queries.append(query)\n",
    "            expandedQuery1.append(query + ', ' + query_2_click_doc[sid_query]['expanded_keywords'][0])\n",
    "            expandedQuery2.append(query + ', ' + query_2_click_doc[sid_query]['expanded_keywords'][1])\n",
    "            expandedQuery3.append(query + ', ' + query_2_click_doc[sid_query]['expanded_keywords'][2])\n",
    "            keys.append(sid_query)\n",
    "\n",
    "        embs = self.model.encode(queries, batch_size=128, show_progress_bar=False, device=DEVICE)\n",
    "        embs1 = self.model.encode(expandedQuery1, batch_size=128, show_progress_bar=False, device=DEVICE)\n",
    "        embs2 = self.model.encode(expandedQuery2, batch_size=128, show_progress_bar=False, device=DEVICE)\n",
    "        embs3 = self.model.encode(expandedQuery3, batch_size=128, show_progress_bar=False, device=DEVICE)\n",
    "        query_2_embedding = dict(zip(keys, embs))\n",
    "        expanded1_2_embedding = dict(zip(keys, embs1))\n",
    "        expanded2_2_embedding = dict(zip(keys, embs2))\n",
    "        expanded3_2_embedding = dict(zip(keys, embs3))\n",
    "\n",
    "        del queries\n",
    "        del embs\n",
    "        del embs1\n",
    "        del embs2\n",
    "        del embs3\n",
    "\n",
    "        # output variables\n",
    "        run = defaultdict(dict)\n",
    "        qrel = defaultdict(dict)\n",
    "\n",
    "        relevances = []\n",
    "\n",
    "        for sidquery in list(query_2_click_doc.keys()):\n",
    "\n",
    "            # query embedding\n",
    "            query_vector = query_2_embedding[sidquery]\n",
    "            expanded_query_vector = (expanded1_2_embedding[sidquery] + expanded2_2_embedding[sidquery] + expanded3_2_embedding[sidquery]) / 3\n",
    "            expanded_query_vector = expanded_query_vector.astype('float32')\n",
    "\n",
    "            labels, distances = self.rerank(query_vector, expanded_query_vector, depth)\n",
    "            \n",
    "            # output prediction\n",
    "            for i in range(self.top_k):\n",
    "                run[sidquery][str(labels[0][i])] = self.top_k - i\n",
    "\n",
    "            # for i in range(len(labels[0])):\n",
    "            # run[sidquery][str(labels[0][i])] = 1 - distances[0][i]\n",
    "\n",
    "            if self.ifRelevance:  \n",
    "                relevances.append(self.evalRelevanceList(query_2_click_doc[sid_query]['query'], labels[0][:10]))\n",
    "            else:\n",
    "                relevances.append(0)\n",
    "\n",
    "            # output ground truth\n",
    "            for eid, rank in query_2_click_doc[sidquery]['click_doc_list']:\n",
    "                qrel[sidquery][str(eid)] = 1\n",
    "\n",
    "        evaluator = pytrec_eval.RelevanceEvaluator(qrel, {'map_cut_10', 'ndcg_cut_10', 'ndcg_cut_20', \\\n",
    "                                                          'recall_10', 'recall_50', 'recall_200', 'P_10', 'P_100', 'recip_rank'})\n",
    "        result = evaluator.evaluate(run)\n",
    "        avg_map = round(sum([sub_dict['map_cut_10'] for sub_dict in result.values()]) / len(result), 3)\n",
    "        avg_ndcg_10 = round(sum([sub_dict['ndcg_cut_10'] for sub_dict in result.values()]) / len(result), 3)\n",
    "        avg_ndcg_20 = round(sum([sub_dict['ndcg_cut_20'] for sub_dict in result.values()]) / len(result), 3)\n",
    "        avg_recall_10 = round(sum([sub_dict['recall_10'] for sub_dict in result.values()]) / len(result), 3)\n",
    "        avg_recall_50 = round(sum([sub_dict['recall_50'] for sub_dict in result.values()]) / len(result), 3)\n",
    "        avg_recall_200 = round(sum([sub_dict['recall_200'] for sub_dict in result.values()]) / len(result), 3)\n",
    "        avg_precision_10 = round(sum([sub_dict['P_10'] for sub_dict in result.values()]) / len(result), 3)\n",
    "        avg_precision_100 = round(sum([sub_dict['P_100'] for sub_dict in result.values()]) / len(result), 3)\n",
    "        avg_mrr = round(sum([sub_dict['recip_rank'] for sub_dict in result.values()]) / len(result), 3)\n",
    "\n",
    "        relevance_10 = round(sum(relevances) / len(result), 3)\n",
    "\n",
    "        # print(f\"MAP@10: {avg_map}, NDCG@10: {avg_ndcg_10}, NDCG@20: {avg_ndcg_20}, Recall@10:{avg_recall_10}, @50: {avg_recall_50}, @100: {avg_recall_100}\")\n",
    "        overall_result = {'MAP@10': avg_map, 'NDCG@10': avg_ndcg_10, 'NDCG@20': avg_ndcg_20, \\\n",
    "                          'Recall@10': avg_recall_10, 'Recall@50': avg_recall_50, 'Recall@200': avg_recall_200, 'Precision@10': avg_precision_10, 'Precision@100': avg_precision_100, 'MRR': avg_mrr, 'Relavance@10': relevance_10}\n",
    "\n",
    "        return result, overall_result\n",
    "\n",
    "    \n",
    "    def evalExpandedPerformance(self, query_2_click_doc: dict, key: str='expanded_keywords'):\n",
    "        queries = []\n",
    "        expandedQuery1 = []\n",
    "        expandedQuery2 = []\n",
    "        expandedQuery3 = []\n",
    "        keys = []\n",
    "        for sid_query in query_2_click_doc:\n",
    "            query = query_2_click_doc[sid_query]['query']\n",
    "            queries.append(query)\n",
    "            expandedQuery1.append(query + ', ' + query_2_click_doc[sid_query][key][0])\n",
    "            expandedQuery2.append(query + ', ' + query_2_click_doc[sid_query][key][1])\n",
    "            expandedQuery3.append(query + ', ' + query_2_click_doc[sid_query][key][2])\n",
    "            keys.append(sid_query)\n",
    "\n",
    "        embs = self.model.encode(queries, batch_size=128, show_progress_bar=False, device=DEVICE)\n",
    "        embs1 = self.model.encode(expandedQuery1, batch_size=128, show_progress_bar=False, device=DEVICE)\n",
    "        embs2 = self.model.encode(expandedQuery2, batch_size=128, show_progress_bar=False, device=DEVICE)\n",
    "        embs3 = self.model.encode(expandedQuery3, batch_size=128, show_progress_bar=False, device=DEVICE)\n",
    "        query_2_embedding = dict(zip(keys, embs))\n",
    "        expanded1_2_embedding = dict(zip(keys, embs1))\n",
    "        expanded2_2_embedding = dict(zip(keys, embs2))\n",
    "        expanded3_2_embedding = dict(zip(keys, embs3))\n",
    "\n",
    "        del queries\n",
    "        del embs\n",
    "        del embs1\n",
    "        del embs2\n",
    "        del embs3\n",
    "\n",
    "        # output variables\n",
    "        run = defaultdict(dict)\n",
    "        qrel = defaultdict(dict)\n",
    "\n",
    "\n",
    "        for sidquery in list(query_2_click_doc.keys()):\n",
    "\n",
    "            # query embedding\n",
    "            query_vector = (expanded1_2_embedding[sidquery] + expanded2_2_embedding[sidquery] + expanded3_2_embedding[sidquery]) / 3\n",
    "            query_vector = query_vector.astype('float32')\n",
    "\n",
    "            labels, distances = self.searching_index.knn_query(query_vector, k=self.top_k)\n",
    "            \n",
    "            # output prediction\n",
    "            for i in range(self.top_k):\n",
    "                run[sidquery][str(labels[0][i])] = self.top_k - i\n",
    "\n",
    "            # for i in range(len(labels[0])):\n",
    "            # run[sidquery][str(labels[0][i])] = 1 - distances[0][i]\n",
    "\n",
    "            # output ground truth\n",
    "            for eid, rank in query_2_click_doc[sidquery]['click_doc_list']:\n",
    "                qrel[sidquery][str(eid)] = 1\n",
    "\n",
    "        evaluator = pytrec_eval.RelevanceEvaluator(qrel, {'map_cut_10', 'ndcg_cut_10', 'ndcg_cut_20', \\\n",
    "                                                          'recall_10', 'recall_50', 'recall_200', 'P_10', 'P_100', 'recip_rank'})\n",
    "        result = evaluator.evaluate(run)\n",
    "        avg_map = round(sum([sub_dict['map_cut_10'] for sub_dict in result.values()]) / len(result), 3)\n",
    "        avg_ndcg_10 = round(sum([sub_dict['ndcg_cut_10'] for sub_dict in result.values()]) / len(result), 3)\n",
    "        avg_ndcg_20 = round(sum([sub_dict['ndcg_cut_20'] for sub_dict in result.values()]) / len(result), 3)\n",
    "        avg_recall_10 = round(sum([sub_dict['recall_10'] for sub_dict in result.values()]) / len(result), 3)\n",
    "        avg_recall_50 = round(sum([sub_dict['recall_50'] for sub_dict in result.values()]) / len(result), 3)\n",
    "        avg_recall_200 = round(sum([sub_dict['recall_200'] for sub_dict in result.values()]) / len(result), 3)\n",
    "        avg_precision_10 = round(sum([sub_dict['P_10'] for sub_dict in result.values()]) / len(result), 3)\n",
    "        avg_precision_100 = round(sum([sub_dict['P_100'] for sub_dict in result.values()]) / len(result), 3)\n",
    "        avg_mrr = round(sum([sub_dict['recip_rank'] for sub_dict in result.values()]) / len(result), 3)\n",
    "\n",
    "        # print(f\"MAP@10: {avg_map}, NDCG@10: {avg_ndcg_10}, NDCG@20: {avg_ndcg_20}, Recall@10:{avg_recall_10}, @50: {avg_recall_50}, @100: {avg_recall_100}\")\n",
    "        overall_result = {'MAP@10': avg_map, 'NDCG@10': avg_ndcg_10, 'NDCG@20': avg_ndcg_20, \\\n",
    "                          'Recall@10': avg_recall_10, 'Recall@50': avg_recall_50, 'Recall@200': avg_recall_200, 'Precision@10': avg_precision_10, 'Precision@100': avg_precision_100, 'MRR': avg_mrr}\n",
    "\n",
    "        return result, overall_result\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        print(\"===Stage 1: Author Representation===\")\n",
    "        self.extractAuthorRepresentation()\n",
    "        self.storeData(filename = 'data.pickle')\n",
    "\n",
    "        print(\"===Stage 2: Personalised Expansion===\")\n",
    "        self.personalisedExpandQuery()\n",
    "        self.storeData(filename = 'data.pickle')\n",
    "\n",
    "        print(\"===Stage 3: Expand===\")\n",
    "        self.expandQuery()\n",
    "        self.storeData(filename = 'data.pickle')\n",
    "\n",
    "        print(\"===Stage 4: Evaluate===\")\n",
    "        origianlQuery = copy.deepcopy(self.data)\n",
    "        # print(\"The performance of original queries: \")\n",
    "        original_result, overall = self.evalPerformance(origianlQuery, 'query')\n",
    "        overall['Items'] = \"Original Query\"\n",
    "        self.overall_result = pd.concat([self.overall_result, pd.DataFrame.from_records([overall])])\n",
    "\n",
    "        expanded_queries = copy.deepcopy(origianlQuery)\n",
    "        for key, value in expanded_queries.items():\n",
    "            value['query'] += ', ' + value['answer_query']\n",
    "        # print(f\"The performance of adding expansion without authour's information: \")\n",
    "        without_info_result, overall = self.evalPerformance(expanded_queries, 'query')\n",
    "        overall['Items'] = 'Expansion'\n",
    "\n",
    "        self.overall_result.loc[len(self.overall_result)] = overall\n",
    "\n",
    "        keywords_results = []\n",
    "        for i in range(3):\n",
    "            expanded_queries = copy.deepcopy(origianlQuery)\n",
    "            for key, value in expanded_queries.items():\n",
    "                for k in range(i + 1):\n",
    "                    value['query'] += ', ' + value['expanded_keywords'][k]\n",
    "            # print(f\"The performance of adding {i+1} keywords: \")\n",
    "            expanded_result, overall = self.evalPerformance(expanded_queries, 'query')\n",
    "            keywords_results.append(expanded_result)\n",
    "            # overall['Items'] = f'Personalised Expansion {i + 1} Keywords'\n",
    "            # self.overall_result.loc[len(self.overall_result)] = overall\n",
    "\n",
    "        expanded_queries = copy.deepcopy(origianlQuery)\n",
    "        expanded_result, overall = self.evalExpandedPerformance(expanded_queries)\n",
    "        overall['Items'] = 'Personalised Expansion'\n",
    "        self.overall_result.loc[len(self.overall_result)] = overall\n",
    "\n",
    "        expanded_queries = copy.deepcopy(origianlQuery)\n",
    "        for key, value in expanded_queries.items():\n",
    "            for k in range(len(value['expanded_keywords'])):\n",
    "                value['query'] += ', ' + value['expanded_keywords'][k]\n",
    "        # print(f\"The performance of adding all keywords: \")\n",
    "        all_result, overall = self.evalPerformance(expanded_queries, 'query')\n",
    "        # overall['Items'] = 'Personalised Expansion All Keywords'\n",
    "        # self.overall_result.loc[len(self.overall_result)] = overall\n",
    "\n",
    "        for key, value in self.data.items():\n",
    "            expandedQuery = value['query'] + ' and ' + value['answer_query']\n",
    "\n",
    "            expandedQuery1 = value['query'] + ' and ' + value['expanded_keywords'][0]\n",
    "            expandedQuery2 = expandedQuery1 + ' and ' + value['expanded_keywords'][1]\n",
    "            expandedQuery3 = expandedQuery2 + ' and ' + value['expanded_keywords'][2]\n",
    "\n",
    "            expandedQueryAll = value['query']\n",
    "            for k in range(len(value['expanded_keywords'])):\n",
    "                expandedQueryAll += ' and ' + value['expanded_keywords'][k]\n",
    "\n",
    "            newRow = pd.Series([value['query'], original_result[key]['ndcg_cut_10'], \\\n",
    "                                expandedQuery, without_info_result[key]['ndcg_cut_10'], \\\n",
    "                                expandedQuery1, keywords_results[0][key]['ndcg_cut_10'], \\\n",
    "                                expandedQuery2, keywords_results[1][key]['ndcg_cut_10'], \\\n",
    "                                expandedQuery3, keywords_results[2][key]['ndcg_cut_10'], \\\n",
    "                                expandedQueryAll, all_result[key]['ndcg_cut_10'], expanded_result[key]['ndcg_cut_10']], index=self.result.columns)\n",
    "            self.result = pd.concat([self.result, newRow.to_frame().T], ignore_index=True)\n",
    "\n",
    "    def run_rerank(self, depth: int):\n",
    "        print(\"===Stage 5: Re-rank===\")\n",
    "        origianlQuery = copy.deepcopy(self.data)\n",
    "        # print(\"The performance of original queries: \")\n",
    "        original_result, overall = self.evalRerankPerformance(origianlQuery, 'query', depth)\n",
    "        overall['Items'] = \"RERANK - Original Query\"\n",
    "        self.overall_result = pd.concat([self.overall_result, pd.DataFrame.from_records([overall])])\n",
    "\n",
    "        expanded_queries = copy.deepcopy(origianlQuery)\n",
    "        for key, value in expanded_queries.items():\n",
    "            value['expanded_query'] = value['query'] + ', ' + value['answer_query']\n",
    "        # print(f\"The performance of adding expansion without authour's information: \")\n",
    "        without_info_result, overall = self.evalRerankPerformance(expanded_queries, 'expanded_query', depth)\n",
    "        overall['Items'] = \"RERANK - Expansion\"\n",
    "        self.overall_result = pd.concat([self.overall_result, pd.DataFrame.from_records([overall])])\n",
    "\n",
    "        keywords_results = []\n",
    "        for i in range(3):\n",
    "            expanded_queries = copy.deepcopy(origianlQuery)\n",
    "            for key, value in expanded_queries.items():\n",
    "                value['expanded_query'] = value['query']\n",
    "                for k in range(i + 1):\n",
    "                    value['expanded_query'] += ', ' + value['expanded_keywords'][k]\n",
    "            # print(f\"The performance of adding {i+1} keywords: \")\n",
    "            expanded_result, overall = self.evalRerankPerformance(expanded_queries, 'expanded_query', depth)\n",
    "            keywords_results.append(expanded_result)\n",
    "            # overall['Items'] = f'RERANK - Personalised Expansion {i + 1} Keywords'\n",
    "            # self.overall_result.loc[len(self.overall_result)] = overall\n",
    "\n",
    "        expanded_queries = copy.deepcopy(origianlQuery)\n",
    "        expanded_result, overall = self.evalRerunkExpandedPerformance(expanded_queries, depth)\n",
    "        overall['Items'] = 'RERANK - Personalised Expansion'\n",
    "        self.overall_result.loc[len(self.overall_result)] = overall\n",
    "\n",
    "        expanded_queries = copy.deepcopy(origianlQuery)\n",
    "        for key, value in expanded_queries.items():\n",
    "            value['expanded_query'] = value['query']\n",
    "            for k in range(len(value['expanded_keywords'])):\n",
    "                value['expanded_query'] += ', ' + value['expanded_keywords'][k]\n",
    "        all_result, overall = self.evalRerankPerformance(expanded_queries, 'expanded_query', depth)\n",
    "        # overall['Items'] = 'RERANK - Personalised Expansion All Keywords'\n",
    "        # self.overall_result.loc[len(self.overall_result)] = overall"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "utils",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
